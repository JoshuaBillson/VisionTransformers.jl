<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · VisionTransformers.jl</title><meta name="title" content="Home · VisionTransformers.jl"/><meta property="og:title" content="Home · VisionTransformers.jl"/><meta property="twitter:title" content="Home · VisionTransformers.jl"/><meta name="description" content="Documentation for VisionTransformers.jl."/><meta property="og:description" content="Documentation for VisionTransformers.jl."/><meta property="twitter:description" content="Documentation for VisionTransformers.jl."/><meta property="og:url" content="https://JoshuaBillson.github.io/VisionTransformers.jl/"/><meta property="twitter:url" content="https://JoshuaBillson.github.io/VisionTransformers.jl/"/><link rel="canonical" href="https://JoshuaBillson.github.io/VisionTransformers.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>VisionTransformers.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Models"><span>Models</span></a></li><li class="toplevel"><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li class="toplevel"><a class="tocitem" href="#Utilities"><span>Utilities</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JoshuaBillson/VisionTransformers.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="VisionTransformers"><a class="docs-heading-anchor" href="#VisionTransformers">VisionTransformers</a><a id="VisionTransformers-1"></a><a class="docs-heading-anchor-permalink" href="#VisionTransformers" title="Permalink"></a></h1><p><a href="https://github.com/JoshuaBillson/VisionTransformers.jl">VisionTransformers</a> is a pure Julia package implementing various vision transformer models in Flux..</p><ul><li><a href="#VisionTransformers.ConvAttention"><code>VisionTransformers.ConvAttention</code></a></li><li><a href="#VisionTransformers.MultiHeadAttention"><code>VisionTransformers.MultiHeadAttention</code></a></li><li><a href="#VisionTransformers.SRAttention"><code>VisionTransformers.SRAttention</code></a></li><li><a href="#VisionTransformers.WindowedAttention"><code>VisionTransformers.WindowedAttention</code></a></li><li><a href="#VisionTransformers.CvT"><code>VisionTransformers.CvT</code></a></li><li><a href="#VisionTransformers.MLP"><code>VisionTransformers.MLP</code></a></li><li><a href="#VisionTransformers.PVT"><code>VisionTransformers.PVT</code></a></li><li><a href="#VisionTransformers.SWIN"><code>VisionTransformers.SWIN</code></a></li><li><a href="#VisionTransformers.ViT"><code>VisionTransformers.ViT</code></a></li><li><a href="#VisionTransformers.img2seq"><code>VisionTransformers.img2seq</code></a></li><li><a href="#VisionTransformers.seq2img"><code>VisionTransformers.seq2img</code></a></li></ul><h1 id="Models"><a class="docs-heading-anchor" href="#Models">Models</a><a id="Models-1"></a><a class="docs-heading-anchor-permalink" href="#Models" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.ViT" href="#VisionTransformers.ViT"><code>VisionTransformers.ViT</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ViT(config::Symbol; kw...)
ViT(dim::Integer, nheads::Integer, depth::Integer;
    imsize=(224,224), patchsize=(16,16), inchannels=3,
    nclasses=1000, mlp_ratio=4, qkv_bias=true,
    dropout=0.1, drop_path=0.0, class_token=false)</code></pre><p>Construct a Vision Transformer (ViT) model for image classification.   The input image is split into non-overlapping patches, each patch is linearly embedded into a <code>dim</code>-dimensional vector, and a sequence of Transformer blocks is applied. Optionally, a learnable class token may be prepended to the patch sequence for classification.</p><p><strong>Arguments</strong></p><ul><li><code>config</code>: One of <code>:tiny</code>, <code>:small</code>, <code>:base</code>, <code>:large</code>, or <code>:huge</code>.</li><li><code>dim</code>: Embedding dimension of the patch tokens.</li><li><code>nheads</code>: Number of attention heads in each Transformer block.</li><li><code>depth</code>: Number of Transformer blocks in the encoder.</li><li><code>imsize</code>: Input image size <code>(height, width)</code>. Default <code>(224,224)</code>.</li><li><code>patchsize</code>: Patch size <code>(height, width)</code>. Default <code>(16,16)</code>.</li><li><code>inchannels</code>: Number of input image channels. Default is <code>3</code>.</li><li><code>nclasses</code>: Number of output classes for classification. Default is <code>1000</code>.</li><li><code>mlp_ratio</code>: Expansion ratio for the hidden dimension of the MLP relative to <code>dim</code>. Default is <code>4</code>.</li><li><code>qkv_bias</code>: Whether to add a bias to query, key, and value projections. Default is <code>true</code>.</li><li><code>dropout</code>: Dropout probability applied to embeddings, MLP, and attention outputs.</li><li><code>drop_path</code>: Probability for stochastic depth (drop-path) regularization.</li><li><code>class_token</code>: Whether to prepend a learnable class token to the patch sequence for classification. If <code>false</code>, global average pooling is applied instead.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/models/vit.jl#L1-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.CvT" href="#VisionTransformers.CvT"><code>VisionTransformers.CvT</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CvT(config::Symbol; kw...)
CvT(dim::Int, depths, nheads;
    inchannels=3, mlp_ratio=4,
    dropout=0.1, drop_path=0.0,
    nclasses=1000)</code></pre><p>Construct a Convolutional Vision Transformer (CvT) model for image classification. CvT extends the Vision Transformer (ViT) by replacing linear projections with convolutional projections and by introducing convolutional token embeddings. This design enhances locality and translation invariance while retaining the benefits of Transformer-based global modeling.</p><p><strong>Arguments</strong></p><ul><li><code>config</code>: One of <code>:B13</code>, <code>:B21</code>, or <code>:W24</code>.</li><li><code>dim</code>: Embedding dimension of patch tokens in the first stage.</li><li><code>depths</code>: Number of Transformer blocks in each stage.</li><li><code>nheads</code>: Number of attention heads in each stage.</li><li><code>inchannels</code>: Number of input image channels. Default is <code>3</code>.</li><li><code>mlp_ratio:</code>: Expansion ratio for the hidden dimension of the MLP relative to <code>dim</code>. Default is <code>4</code>.</li><li><code>dropout</code>: Dropout probability applied to embeddings, MLP, and attention outputs.</li><li><code>drop_path</code>: Probability for stochastic depth (drop-path) regularization.</li><li><code>nclasses</code>: Number of output classes for classification. Default is <code>1000</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/models/cvt.jl#L1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.PVT" href="#VisionTransformers.PVT"><code>VisionTransformers.PVT</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PVT(config::Symbol; kw...)
PVT(embed_dims, depths, nheads, mlp_ratios, sr_ratios;
    qkv_bias=true, dropout=0.1, drop_path=0.0,
    imsize=224, inchannels=3, nclasses=1000)</code></pre><p>Construct a Pyramid Vision Transformer (PVT) model for image classification or feature extraction. PVT builds a hierarchical representation by progressively reducing spatial resolution while increasing the embedding dimension. Spatial Reduction Attention (SRA) is used to make attention efficient on high-resolution feature maps.</p><p><strong>Arguments</strong></p><ul><li><code>config</code>: One of <code>:tiny</code>, <code>:small</code>, <code>:medium</code>, or <code>:large</code>.</li><li><code>embed_dims</code>: Embedding dimension for each stage of the network.</li><li><code>depths</code>: Number of transformer blocks in each stage.</li><li><code>nheads</code>: Number of attention heads in each stage.</li><li><code>mlp_ratios</code>: Expansion ratio for the hidden dimension of the MLP in each stage.</li><li><code>sr_ratios</code>: Spatial reduction ratio for SRA in each stage.</li><li><code>qkv_bias</code>: Whether to add a bias to query, key, and value projections. Default is <code>true</code>.</li><li><code>dropout</code>: Dropout probability applied to MLP and attention outputs.</li><li><code>drop_path</code>: Probability for stochastic depth (drop-path) regularization.</li><li><code>imsize</code>: Input image size (assumed square). Default is <code>224</code>.</li><li><code>inchannels</code>: Number of input image channels. Default is <code>3</code>.</li><li><code>nclasses</code>: Number of output classes for classification. Default is <code>1000</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/models/pvt.jl#L1-L26">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.SWIN" href="#VisionTransformers.SWIN"><code>VisionTransformers.SWIN</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SWIN(config::Symbol; kw...)
SWIN(embed_dims, depths, nheads;
     window_size=7, position_embedding=true, mlp_ratio=4,
     qkv_bias=true, dropout=0.1, drop_path=0.0,
     inchannels=3, nclasses=1000)</code></pre><p>Construct a Swin Transformer model for image classification or feature extraction. The architecture is composed of multiple stages of Shifted Windowed Multi-Head Self-Attention (SW-MSA) and MLP blocks. The model progressively merges patches while increasing channel dimension, enabling hierarchical feature learning.</p><p><strong>Arguments</strong></p><ul><li><code>config</code>: One of <code>:tiny</code>, <code>:small</code>, <code>:base</code>, or <code>:large</code>.</li><li><code>embed_dims</code>: Embedding dimension for each stage of the network.</li><li><code>depths</code>: Number of transformer blocks in each stage.</li><li><code>nheads</code>: Number of attention heads in each stage.</li><li><code>window_size</code>: Spatial size of the attention window. Default is <code>7</code>.</li><li><code>position_embedding</code>: Whether to include learnable relative position embeddings in attention.</li><li><code>mlp_ratio</code>: Expansion ratio for the hidden dimension of the MLP relative to the embedding dimension. Default is <code>4</code>.</li><li><code>qkv_bias</code>: Whether to add bias to query, key, and value projections.</li><li><code>dropout</code>: Dropout probability applied to MLP and attention outputs.</li><li><code>drop_path</code>: Probability for stochastic depth (drop-path) regularization.</li><li><code>inchannels</code>: Number of input image channels. Default is <code>3</code>.</li><li><code>nclasses</code>: Number of output classes for classification. Default is <code>1000</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/models/swin.jl#L1-L27">source</a></section></article><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.MultiHeadAttention" href="#VisionTransformers.MultiHeadAttention"><code>VisionTransformers.MultiHeadAttention</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MultiHeadAttention(dim::Integer;
                   nheads=8,
                   attn_dropout_prob=0.0,
                   proj_dropout_prob=0.0,
                   qkv_bias=false)</code></pre><p>Construct a multi-head self-attention layer. The input features are linearly projected into query, key, and value tensors. Expects tensors of size <code>C x S x N</code>, where <code>C</code> is the embedding dimension, <code>S</code> is the sequence length, and <code>N</code> is the batch size. Note that <code>S</code> can have more than one dimension.</p><p><strong>Arguments</strong></p><ul><li><code>dim</code>: Dimensionality of the input feature embeddings.</li><li><code>nheads</code>: Number of parallel attention heads. Default is <code>8</code>.</li><li><code>attn_dropout_prob</code>: Dropout probability applied to attention weights.</li><li><code>proj_dropout_prob</code>: Dropout probability applied after the output projection.</li><li><code>qkv_bias</code>: Whether to add learnable bias terms to the query, key, and value projections.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/attention.jl#L1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.ConvAttention" href="#VisionTransformers.ConvAttention"><code>VisionTransformers.ConvAttention</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, norm=:BN, qkv_bias=false)
ConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, qkv_bias=false)</code></pre><p>A convolutional attention layer as proposed in CvT.</p><p><strong>Parameters</strong></p><ul><li><code>dim</code>: The dimension of the feature embedding.</li><li><code>nheads</code>: The number of heads to use for self attention.</li><li><code>kernel</code>: The kernel size used in the convolutional projection layers.</li><li><code>attn_dropout_prob</code>: Dropout probability in the attention block.</li><li><code>proj_dropout_prob</code>: Dropout probability in the projection block.</li><li><code>q_stride</code>: Convolutional stride used to compute the query.</li><li><code>kv_stride</code>: Convolutional stride used to compute the key and value.</li><li><code>qkv_bias</code>: Whether to include a bias term in the convolutional projection layers.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/attention.jl#L66-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.SRAttention" href="#VisionTransformers.SRAttention"><code>VisionTransformers.SRAttention</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SRAttention(dim::Int;
            nheads=8,
            qkv_bias=false,
            attn_dropout_prob=0.0,
            proj_dropout_prob=0.0,
            sr_ratio=(1,1),
            sr_method=:conv)</code></pre><p>Construct a Spatial Reduction Attention (SRA) layer as used in PVT and Twins. This is a variant of multi-head self-attention designed to reduce computational cost on high-resolution feature maps by  spatially downsampling the keys and values before attention. The reduction can be performed using  convolutional or pooling methods.</p><p><strong>Arguments</strong></p><ul><li><code>dim</code>: Dimensionality of the input feature embeddings.</li><li><code>nheads</code>: Number of parallel attention heads. Default is <code>8</code>.</li><li><code>qkv_bias</code>: Whether to add learnable bias terms to query, key, and value projections.</li><li><code>attn_dropout_prob</code>: Dropout probability applied to the attention weights.</li><li><code>proj_dropout_prob</code>: Dropout probability applied after the output projection.</li><li><code>sr_ratio</code>: Spatial reduction ratio <code>(height, width)</code> applied to keys and values. <code>(1,1)</code> means no reduction.</li><li><code>sr_method</code>: Method used for spatial reduction. Must be one of <code>:conv</code> or <code>:pool</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/attention.jl#L118-L140">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.WindowedAttention" href="#VisionTransformers.WindowedAttention"><code>VisionTransformers.WindowedAttention</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">WindowedAttention(dim::Integer; window_size=(7,7), shift_size=(0,0),
                  position_embedding=false, nheads=8, qkv_bias=false,
                  attn_dropout_prob=0.0, proj_dropout_prob=0.0)</code></pre><p>Construct a windowed multi-head self-attention module, as used in Swin Transformers. The input is partitioned into non-overlapping windows of size <code>window_size</code>, and attention is computed within each window. Optionally, the windows can be shifted by <code>shift_size</code> to allow cross-window connections.</p><p><strong>Arguments</strong></p><ul><li><code>dim</code>: Dimensionality of the input feature embeddings.</li><li><code>window_size</code>: Spatial size of each attention window <code>(height, width)</code>.</li><li><code>shift_size</code>: Offset applied to window partitions to enable connections across windows. Default <code>(0,0)</code> means no shift.</li><li><code>position_embedding</code>: Whether to include learnable relative position embeddings in the attention computation.</li><li><code>nheads</code>: Number of attention heads.</li><li><code>qkv_bias</code>: Whether to add learnable bias parameters to the query, key, and value projections.</li><li><code>attn_dropout_prob</code>: Dropout probability applied to the attention weights.</li><li><code>proj_dropout_prob</code>: Dropout probability applied to the output projection.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/attention.jl#L189-L208">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.MLP" href="#VisionTransformers.MLP"><code>VisionTransformers.MLP</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MLP(indims, hiddendims, outdims; dropout=0.0, act=Flux.gelu)</code></pre><p>Build a 2-layer multi-layer-perceptron.</p><p><strong>Parameters</strong></p><ul><li><code>indims</code>: The dimension of the input features.</li><li><code>hiddendims</code>: The dimension of the hidden features.</li><li><code>outdims</code>: The dimension of the output features.</li><li><code>dropout</code>: The dropout probability following each Dense layer.</li><li><code>act</code>: The activation function following the first Dense layer.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/base.jl#L1-L12">source</a></section></article><h1 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.img2seq" href="#VisionTransformers.img2seq"><code>VisionTransformers.img2seq</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">img2seq(x::AbstractArray{&lt;:Any,4})</code></pre><p>Take a collection of image tokens of size [W x H x C x N] and flatten them into a sequence of size [C x L x N] where L = W * H.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/utils.jl#L3-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="VisionTransformers.seq2img" href="#VisionTransformers.seq2img"><code>VisionTransformers.seq2img</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">seq2img(x::AbstractArray{&lt;:Any,3})</code></pre><p>Take a sequence of image tokens of size [C x L x N] and reshape it into an image of size [W x H x C x N], where W = H = sqrt(L).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JoshuaBillson/VisionTransformers.jl/blob/c416ce7aa88e2169ff4e2bbc539589e7c745ca95/src/layers/utils.jl#L15-L20">source</a></section></article></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 10 September 2025 16:02">Wednesday 10 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
