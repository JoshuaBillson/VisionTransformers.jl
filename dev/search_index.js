var documenterSearchIndex = {"docs":
[{"location":"#VisionTransformers","page":"Home","title":"VisionTransformers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for VisionTransformers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#VisionTransformers.ConvAttention","page":"Home","title":"VisionTransformers.ConvAttention","text":"ConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, norm=:BN, qkv_bias=false)\nConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, qkv_bias=false)\n\nA convolutional attention layer as proposed in CVT.\n\nParameters\n\ndim: The dimension of the feature embedding.\nnheads: The number of heads to use for self attention.\nkernel: The kernel size used in the convolutional projection layers.\nattn_dropout_prob: Dropout probability in the attention block.\nproj_dropout_prob: Dropout probability in the projection block.\nq_stride: Convolutional stride used to compute the query.\nkv_stride: Convolutional stride used to compute the key and value.\nqkv_bias: Whether to include a bias term in the convolutional projection layers.\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.MultiHeadAttention","page":"Home","title":"VisionTransformers.MultiHeadAttention","text":"MultiHeadAttentionModule(qkv, proj; nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0)\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.WindowedAttention-Tuple{Integer}","page":"Home","title":"VisionTransformers.WindowedAttention","text":"WindowedAttention(dim::Integer; window_size=(7,7), shift_size=(0,0),\n                  position_embedding=false, nheads=8, qkv_bias=false,\n                  attn_dropout_prob=0.0, proj_dropout_prob=0.0)\n\nConstruct a windowed multi-head self-attention module, as used in Swin Transformers. The input is partitioned into non-overlapping windows of size window_size, and attention is computed within each window. Optionally, the windows can be shifted by shift_size to allow cross-window connections.\n\nArguments\n\ndim: Dimensionality of the input feature embeddings.\nwindow_size: Spatial size of each attention window (height, width).\nshift_size: Offset applied to window partitions to enable connections across windows. Default (0,0) means no shift.\nposition_embedding: Whether to include learnable relative position embeddings in the attention computation.\nnheads: Number of attention heads.\nqkv_bias: Whether to add learnable bias parameters to the query, key, and value projections.\nattn_dropout_prob: Dropout probability applied to the attention weights.\nproj_dropout_prob: Dropout probability applied to the output projection.\n\n\n\n\n\n","category":"method"},{"location":"#VisionTransformers.MLP-Tuple{Any, Any, Any}","page":"Home","title":"VisionTransformers.MLP","text":"MLP(indims, hiddendims, outdims; dropout=0.0, act=Flux.gelu)\n\nBuild a 2-layer multi-layer-perceptron.\n\nParameters\n\nindims: The dimension of the input features.\nhiddendims: The dimension of the hidden features.\noutdims: The dimension of the output features.\ndropout: The dropout probability following each Dense layer.\nact: The activation function following the first Dense layer.\n\n\n\n\n\n","category":"method"},{"location":"#VisionTransformers.SWIN-Tuple{Symbol}","page":"Home","title":"VisionTransformers.SWIN","text":"SWIN(config::Symbol; kw...)\nSWIN(embed_dims, depths, nheads;\n     window_size=7, position_embedding=true, mlp_ratio=4,\n     qkv_bias=true, dropout=0.1, drop_path=0.0,\n     inchannels=3, nclasses=1000)\n\nConstruct a Swin Transformer model for image classification or feature extraction. The architecture is composed of multiple stages of Shifted Windowed Multi-Head Self-Attention (SW-MSA) and MLP blocks. The model progressively merges patches while increasing channel dimension, enabling hierarchical feature learning.\n\nArguments\n\nconfig: One of :tiny, :small, :base, or :large.\nembed_dims: Embedding dimension for each stage of the network.\ndepths: Number of transformer blocks in each stage.\nnheads: Number of attention heads in each stage.\nwindow_size: Spatial size of the attention window. Default is 7.\nposition_embedding: Whether to include learnable relative position embeddings in attention.\nmlp_ratio: Expansion ratio for the hidden dimension of the MLP relative to the embedding dimension. Default is 4.\nqkv_bias: Whether to add bias to query, key, and value projections.\ndropout: Dropout probability applied to MLP and attention outputs.\ndrop_path: Probability for stochastic depth (drop-path) regularization.\ninchannels: Number of input image channels. Default is 3.\nnclasses: Number of output classes for classification. Default is 1000.\n\n\n\n\n\n","category":"method"},{"location":"#VisionTransformers.img2seq-Union{Tuple{AbstractArray{<:Any, N}}, Tuple{N}} where N","page":"Home","title":"VisionTransformers.img2seq","text":"img2seq(x::AbstractArray{<:Any,4})\n\nTake a collection of image tokens of size [W x H x C x N] and flatten them into a sequence of size [C x L x N] where L = W * H.\n\n\n\n\n\n","category":"method"},{"location":"#VisionTransformers.seq2img","page":"Home","title":"VisionTransformers.seq2img","text":"seq2img(x::AbstractArray{<:Any,3})\n\nTake a sequence of image tokens of size [C x L x N] and reshape it into an image of size [W x H x C x N], where W = H = sqrt(L).\n\n\n\n\n\n","category":"function"}]
}
