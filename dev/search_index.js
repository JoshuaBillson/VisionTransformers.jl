var documenterSearchIndex = {"docs":
[{"location":"#VisionTransformers","page":"Home","title":"VisionTransformers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for VisionTransformers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#VisionTransformers.ConvAttention","page":"Home","title":"VisionTransformers.ConvAttention","text":"ConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, norm=:BN, qkv_bias=false)\nConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, qkv_bias=false)\n\nA convolutional attention layer as proposed in CVT.\n\nParameters\n\ndim: The dimension of the feature embedding.\nnheads: The number of heads to use for self attention.\nkernel: The kernel size used in the convolutional projection layers.\nattn_dropout_prob: Dropout probability in the attention block.\nproj_dropout_prob: Dropout probability in the projection block.\nq_stride: Convolutional stride used to compute the query.\nkv_stride: Convolutional stride used to compute the key and value.\nqkv_bias: Whether to include a bias term in the convolutional projection layers.\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.MultiHeadAttention","page":"Home","title":"VisionTransformers.MultiHeadAttention","text":"MultiHeadAttentionModule(qkv, proj; nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0)\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.MLP-Tuple{Any, Any, Any}","page":"Home","title":"VisionTransformers.MLP","text":"MLP(indims, hiddendims, outdims; dropout=0.0, act=Flux.gelu)\n\nBuild a 2-layer multi-layer-perceptron.\n\nParameters\n\nindims: The dimension of the input features.\nhiddendims: The dimension of the hidden features.\noutdims: The dimension of the output features.\ndropout: The dropout probability following each Dense layer.\nact: The activation function following the first Dense layer.\n\n\n\n\n\n","category":"method"},{"location":"#VisionTransformers.img2seq-Union{Tuple{AbstractArray{<:Any, N}}, Tuple{N}} where N","page":"Home","title":"VisionTransformers.img2seq","text":"img2seq(x::AbstractArray{<:Any,4})\n\nTake a collection of image tokens of size [W x H x C x N] and flatten them into a sequence of size [C x L x N] where L = W * H.\n\n\n\n\n\n","category":"method"},{"location":"#VisionTransformers.seq2img","page":"Home","title":"VisionTransformers.seq2img","text":"seq2img(x::AbstractArray{<:Any,3})\n\nTake a sequence of image tokens of size [C x L x N] and reshape it into an image of size [W x H x C x N], where W = H = sqrt(L).\n\n\n\n\n\n","category":"function"}]
}
