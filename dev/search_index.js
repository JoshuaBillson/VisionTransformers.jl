var documenterSearchIndex = {"docs":
[{"location":"#VisionTransformers","page":"Home","title":"VisionTransformers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for VisionTransformers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Models","page":"Home","title":"Models","text":"","category":"section"},{"location":"#VisionTransformers.ViT","page":"Home","title":"VisionTransformers.ViT","text":"ViT(config::Symbol; kw...)\nViT(dim::Integer, nheads::Integer, depth::Integer;\n    imsize=(224,224), patchsize=(16,16), inchannels=3,\n    nclasses=1000, mlp_ratio=4, qkv_bias=true,\n    dropout=0.1, drop_path=0.0, class_token=false)\n\nConstruct a Vision Transformer (ViT) model for image classification.   The input image is split into non-overlapping patches, each patch is linearly embedded into a dim-dimensional vector, and a sequence of Transformer blocks is applied. Optionally, a learnable class token may be prepended to the patch sequence for classification.\n\nArguments\n\nconfig: One of :tiny, :small, :base, :large, or :huge.\ndim: Embedding dimension of the patch tokens.\nnheads: Number of attention heads in each Transformer block.\ndepth: Number of Transformer blocks in the encoder.\nimsize: Input image size (height, width). Default (224,224).\npatchsize: Patch size (height, width). Default (16,16).\ninchannels: Number of input image channels. Default is 3.\nnclasses: Number of output classes for classification. Default is 1000.\nmlp_ratio: Expansion ratio for the hidden dimension of the MLP relative to dim. Default is 4.\nqkv_bias: Whether to add a bias to query, key, and value projections. Default is true.\ndropout: Dropout probability applied to embeddings, MLP, and attention outputs.\ndrop_path: Probability for stochastic depth (drop-path) regularization.\nclass_token: Whether to prepend a learnable class token to the patch sequence for classification. If false, global average pooling is applied instead.\n\n\n\n\n\n","category":"function"},{"location":"#VisionTransformers.CvT","page":"Home","title":"VisionTransformers.CvT","text":"CvT(config::Symbol; kw...)\nCvT(dim::Int, depths, nheads;\n    inchannels=3, mlp_ratio=4,\n    dropout=0.1, drop_path=0.0,\n    nclasses=1000)\n\nConstruct a Convolutional Vision Transformer (CvT) model for image classification. CvT extends the Vision Transformer (ViT) by replacing linear projections with convolutional projections and by introducing convolutional token embeddings. This design enhances locality and translation invariance while retaining the benefits of Transformer-based global modeling.\n\nArguments\n\nconfig: One of :B13, :B21, or :W24.\ndim: Embedding dimension of patch tokens in the first stage.\ndepths: Number of Transformer blocks in each stage.\nnheads: Number of attention heads in each stage.\ninchannels: Number of input image channels. Default is 3.\nmlp_ratio:: Expansion ratio for the hidden dimension of the MLP relative to dim. Default is 4.\ndropout: Dropout probability applied to embeddings, MLP, and attention outputs.\ndrop_path: Probability for stochastic depth (drop-path) regularization.\nnclasses: Number of output classes for classification. Default is 1000.\n\n\n\n\n\n","category":"function"},{"location":"#VisionTransformers.PVT","page":"Home","title":"VisionTransformers.PVT","text":"PVT(config::Symbol; kw...)\nPVT(embed_dims, depths, nheads, mlp_ratios, sr_ratios;\n    qkv_bias=true, dropout=0.1, drop_path=0.0,\n    imsize=224, inchannels=3, nclasses=1000)\n\nConstruct a Pyramid Vision Transformer (PVT) model for image classification or feature extraction. PVT builds a hierarchical representation by progressively reducing spatial resolution while increasing the embedding dimension. Spatial Reduction Attention (SRA) is used to make attention efficient on high-resolution feature maps.\n\nArguments\n\nconfig: One of :tiny, :small, :medium, or :large.\nembed_dims: Embedding dimension for each stage of the network.\ndepths: Number of transformer blocks in each stage.\nnheads: Number of attention heads in each stage.\nmlp_ratios: Expansion ratio for the hidden dimension of the MLP in each stage.\nsr_ratios: Spatial reduction ratio for SRA in each stage.\nqkv_bias: Whether to add a bias to query, key, and value projections. Default is true.\ndropout: Dropout probability applied to MLP and attention outputs.\ndrop_path: Probability for stochastic depth (drop-path) regularization.\nimsize: Input image size (assumed square). Default is 224.\ninchannels: Number of input image channels. Default is 3.\nnclasses: Number of output classes for classification. Default is 1000.\n\n\n\n\n\n","category":"function"},{"location":"#VisionTransformers.SWIN","page":"Home","title":"VisionTransformers.SWIN","text":"SWIN(config::Symbol; kw...)\nSWIN(embed_dims, depths, nheads;\n     window_size=7, position_embedding=true, mlp_ratio=4,\n     qkv_bias=true, dropout=0.1, drop_path=0.0,\n     inchannels=3, nclasses=1000)\n\nConstruct a Swin Transformer model for image classification or feature extraction. The architecture is composed of multiple stages of Shifted Windowed Multi-Head Self-Attention (SW-MSA) and MLP blocks. The model progressively merges patches while increasing channel dimension, enabling hierarchical feature learning.\n\nArguments\n\nconfig: One of :tiny, :small, :base, or :large.\nembed_dims: Embedding dimension for each stage of the network.\ndepths: Number of transformer blocks in each stage.\nnheads: Number of attention heads in each stage.\nwindow_size: Spatial size of the attention window. Default is 7.\nposition_embedding: Whether to include learnable relative position embeddings in attention.\nmlp_ratio: Expansion ratio for the hidden dimension of the MLP relative to the embedding dimension. Default is 4.\nqkv_bias: Whether to add bias to query, key, and value projections.\ndropout: Dropout probability applied to MLP and attention outputs.\ndrop_path: Probability for stochastic depth (drop-path) regularization.\ninchannels: Number of input image channels. Default is 3.\nnclasses: Number of output classes for classification. Default is 1000.\n\n\n\n\n\n","category":"function"},{"location":"#Layers","page":"Home","title":"Layers","text":"","category":"section"},{"location":"#VisionTransformers.MultiHeadAttention","page":"Home","title":"VisionTransformers.MultiHeadAttention","text":"MultiHeadAttention(dim::Integer;\n                   nheads=8,\n                   attn_dropout_prob=0.0,\n                   proj_dropout_prob=0.0,\n                   qkv_bias=false)\n\nConstruct a multi-head self-attention layer. The input features are linearly projected into query, key, and value tensors. Expects tensors of size C x S x N, where C is the embedding dimension, S is the sequence length, and N is the batch size. Note that S can have more than one dimension.\n\nArguments\n\ndim: Dimensionality of the input feature embeddings.\nnheads: Number of parallel attention heads. Default is 8.\nattn_dropout_prob: Dropout probability applied to attention weights.\nproj_dropout_prob: Dropout probability applied after the output projection.\nqkv_bias: Whether to add learnable bias terms to the query, key, and value projections.\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.ConvAttention","page":"Home","title":"VisionTransformers.ConvAttention","text":"ConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, norm=:BN, qkv_bias=false)\nConvAttention(dim::Int; kernel=(3,3), q_stride=(1,1), kv_stride=(1,1), nheads=8, attn_dropout_prob=0.0, proj_dropout_prob=0.0, qkv_bias=false)\n\nA convolutional attention layer as proposed in CvT.\n\nParameters\n\ndim: The dimension of the feature embedding.\nnheads: The number of heads to use for self attention.\nkernel: The kernel size used in the convolutional projection layers.\nattn_dropout_prob: Dropout probability in the attention block.\nproj_dropout_prob: Dropout probability in the projection block.\nq_stride: Convolutional stride used to compute the query.\nkv_stride: Convolutional stride used to compute the key and value.\nqkv_bias: Whether to include a bias term in the convolutional projection layers.\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.SRAttention","page":"Home","title":"VisionTransformers.SRAttention","text":"SRAttention(dim::Int;\n            nheads=8,\n            qkv_bias=false,\n            attn_dropout_prob=0.0,\n            proj_dropout_prob=0.0,\n            sr_ratio=(1,1),\n            sr_method=:conv)\n\nConstruct a Spatial Reduction Attention (SRA) layer as used in PVT and Twins. This is a variant of multi-head self-attention designed to reduce computational cost on high-resolution feature maps by  spatially downsampling the keys and values before attention. The reduction can be performed using  convolutional or pooling methods.\n\nArguments\n\ndim: Dimensionality of the input feature embeddings.\nnheads: Number of parallel attention heads. Default is 8.\nqkv_bias: Whether to add learnable bias terms to query, key, and value projections.\nattn_dropout_prob: Dropout probability applied to the attention weights.\nproj_dropout_prob: Dropout probability applied after the output projection.\nsr_ratio: Spatial reduction ratio (height, width) applied to keys and values. (1,1) means no reduction.\nsr_method: Method used for spatial reduction. Must be one of :conv or :pool.\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.WindowedAttention","page":"Home","title":"VisionTransformers.WindowedAttention","text":"WindowedAttention(dim::Integer; window_size=(7,7), shift_size=(0,0),\n                  position_embedding=false, nheads=8, qkv_bias=false,\n                  attn_dropout_prob=0.0, proj_dropout_prob=0.0)\n\nConstruct a windowed multi-head self-attention module, as used in Swin Transformers. The input is partitioned into non-overlapping windows of size window_size, and attention is computed within each window. Optionally, the windows can be shifted by shift_size to allow cross-window connections.\n\nArguments\n\ndim: Dimensionality of the input feature embeddings.\nwindow_size: Spatial size of each attention window (height, width).\nshift_size: Offset applied to window partitions to enable connections across windows. Default (0,0) means no shift.\nposition_embedding: Whether to include learnable relative position embeddings in the attention computation.\nnheads: Number of attention heads.\nqkv_bias: Whether to add learnable bias parameters to the query, key, and value projections.\nattn_dropout_prob: Dropout probability applied to the attention weights.\nproj_dropout_prob: Dropout probability applied to the output projection.\n\n\n\n\n\n","category":"type"},{"location":"#VisionTransformers.MLP","page":"Home","title":"VisionTransformers.MLP","text":"MLP(indims, hiddendims, outdims; dropout=0.0, act=Flux.gelu)\n\nBuild a 2-layer multi-layer-perceptron.\n\nParameters\n\nindims: The dimension of the input features.\nhiddendims: The dimension of the hidden features.\noutdims: The dimension of the output features.\ndropout: The dropout probability following each Dense layer.\nact: The activation function following the first Dense layer.\n\n\n\n\n\n","category":"function"},{"location":"#Utilities","page":"Home","title":"Utilities","text":"","category":"section"},{"location":"#VisionTransformers.img2seq","page":"Home","title":"VisionTransformers.img2seq","text":"img2seq(x::AbstractArray{<:Any,4})\n\nTake a collection of image tokens of size [W x H x C x N] and flatten them into a sequence of size [C x L x N] where L = W * H.\n\n\n\n\n\n","category":"function"},{"location":"#VisionTransformers.seq2img","page":"Home","title":"VisionTransformers.seq2img","text":"seq2img(x::AbstractArray{<:Any,3})\n\nTake a sequence of image tokens of size [C x L x N] and reshape it into an image of size [W x H x C x N], where W = H = sqrt(L).\n\n\n\n\n\n","category":"function"}]
}
